import carla
import pygame
import numpy as np
from sensor_manager import *
from dynamic_weather import Weather
import queue
from utils import *
import ipdb
import matplotlib.pyplot as plt
import cv2
from steering_noise import generate_temporally_correlated_noise
import datetime
from models.depthanythingv2 import estimate_depth
import open3d as o3d
import time
def initialize_steering_wheel(wheel_name):
    pygame.joystick.init()
    for i in range(pygame.joystick.get_count()):
        joystick = pygame.joystick.Joystick(i)
        joystick.init()
        if wheel_name in joystick.get_name():
            print(f"Steering wheel '{wheel_name}' initialized.")
            return joystick
    raise Exception(f"Steering wheel '{wheel_name}' not found.")


def main():
    simulation_params = {
        'map_name': 'Town01',
        'host': 'localhost',
        'port': 2000,
        'timeout': 60.0
    }

    camera_params = {
    'bloom_intensity': 0.0,  # Intensity for the bloom post-process effect, 0.0 for disabling it
    'fov': 85.7,  # Horizontal field of view in degrees
    'fstop': 2.4,  # Aperture setting, larger values reduce Depth of Field effect
    'image_size_x': 640,  # Image width in pixels
    'image_size_y': 360,  # Image height in pixels
    'iso': 400.0,  # Camera sensor sensitivity
    'gamma': 2.2,  # Target gamma value of the camera
    'lens_flare_intensity': 0.05,  # Intensity for lens flare post-process effect, 0.0 for disabling it
    'sensor_tick': 0.0,  # Simulation seconds between sensor captures (ticks)
    'shutter_speed': 2000.0,  # Camera shutter speed in seconds (1.0/s)
    'enable_postprocess_effects': True
}

    lidar_params = {
    'channels': 32,  # Number of lasers
    'range': 50.0,  # Maximum distance to measure/raycast in meters
    'points_per_second': 200000,  # Points generated by all lasers per second
    'rotation_frequency': 15.0,  # LIDAR rotation frequency
    'upper_fov': 15.0,  # Angle in degrees of the highest laser
    'lower_fov': -25.0,  # Angle in degrees of the lowest laser
    'horizontal_fov': 90.0,  # Horizontal field of view in degrees, 0 - 360
    'atmosphere_attenuation_rate': 0.004,  # Coefficient that measures the LIDAR intensity loss per meter
    'dropoff_general_rate': 0.35,  # General proportion of points that are randomly dropped
    'dropoff_intensity_limit': 0.8,  # Intensity threshold above which no points are dropped
    'dropoff_zero_intensity': 0.4,  # Probability of dropping each point with zero intensity
    'sensor_tick': 0.0,  # Simulation seconds between sensor captures (ticks)
    'noise_stddev': 0.0  # Standard deviation of the noise model for each point along its raycast vector
    }

    imu_params = {
        'noise_accel_stddev_x': 0.02,
        'noise_accel_stddev_y': 0.02,
        'noise_accel_stddev_z': 0.02,
        'noise_gyro_bias_x': 0.0005,
        'noise_gyro_bias_y': 0.0005,
        'noise_gyro_bias_z': 0.0005,
        'noise_gyro_stddev_x': 0.002,
        'noise_gyro_stddev_y': 0.002,
        'noise_gyro_stddev_z': 0.002,
    'noise_seed': 0,  # Initializer for a pseudorandom number generator
    'sensor_tick': 0.0  # Simulation seconds between sensor captures (ticks)
    }
    # Initialize pygame
    pygame.init()
    wheel_name = 'SIMAGIC Alpha Series Wheelbase' 
    try:
        joy = initialize_steering_wheel(wheel_name)
    except Exception as e:
        print(f"Failed to initialize steering wheel: {e}")


    font = get_font()
    display = pygame.display.set_mode((camera_params['image_size_x'], camera_params['image_size_y']), pygame.HWSURFACE | pygame.DOUBLEBUF)
    pygame.display.set_caption("CARLA Simulation")

    client = initialize_carla_client(
        host=simulation_params['host'],
        port=simulation_params['port'],
        timeout=simulation_params['timeout']
    )
    world = client.get_world()
    actor_list = []
    print("Current Map Name:", client.get_world().get_map().name)
    if simulation_params['map_name'] not in client.get_world().get_map().name:
        print(f"Loading map: {simulation_params['map_name']}")
        client.load_world(simulation_params['map_name'])
    else:
        print(f"Map {client.get_world().get_map().name} is already loaded")

    blueprint_library = world.get_blueprint_library()
    weather = Weather(world.get_weather(), teleport_sun=True, stay_ticks=50)
    #set_weather(client, simulation_params['weather_preset'])
    ego_vehicle = spawn_vehicle(world, "vehicle.tesla.model3", world.get_map().get_spawn_points()[0])
    # Sensor organization v3
    #RGB Camera
    camera_rgb_bp = blueprint_library.find('sensor.camera.rgb')
    for key in camera_params:
        camera_rgb_bp.set_attribute(key, str(camera_params[key]))
    camera_rgb = world.spawn_actor(
                camera_rgb_bp,
                carla.Transform(carla.Location(x=0, z=1.7), carla.Rotation(yaw=+00)),
                attach_to=ego_vehicle)
    actor_list.append(camera_rgb)

    # Semantic Segmentation
    camera_semseg = world.spawn_actor(
                blueprint_library.find('sensor.camera.semantic_segmentation'),
                carla.Transform(carla.Location(x=0, z=1.7), carla.Rotation(yaw=+00)),
                attach_to=ego_vehicle)
    actor_list.append(camera_semseg)

    # #LIDAR 
    lidar_bp = blueprint_library.find('sensor.lidar.ray_cast')
    for key in lidar_params:
            lidar_bp.set_attribute(key, str(lidar_params[key]))
    lidar_bp.set_attribute('dropoff_general_rate', lidar_bp.get_attribute('dropoff_general_rate').recommended_values[0])
    lidar_bp.set_attribute('dropoff_intensity_limit', lidar_bp.get_attribute('dropoff_intensity_limit').recommended_values[0])
    lidar_bp.set_attribute('dropoff_zero_intensity', lidar_bp.get_attribute('dropoff_zero_intensity').recommended_values[0])
    lidar = world.spawn_actor(
                lidar_bp,
                carla.Transform(carla.Location(x=-0.1, y=0.0, z=2.3)),
                attach_to=ego_vehicle)
    actor_list.append(lidar)

    #IMU
    img_bp = blueprint_library.find('sensor.other.imu')
    for key in imu_params:
        img_bp.set_attribute(key, str(imu_params[key]))
    imu_sensor = world.spawn_actor(
                img_bp,
                carla.Transform(),
                attach_to=ego_vehicle)
    actor_list.append(imu_sensor)
    
    # Create sensor objects
    # collision_sensor = world.spawn_actor(
    #     blueprint_library.find('sensor.other.collision'),
    #     carla.Transform(),
    #     attach_to=ego_vehicle)
    # actor_list.append(collision_sensor)

    # lane_invasion_sensor = world.spawn_actor(
    #     blueprint_library.find('sensor.other.lane_invasion'),
    #     carla.Transform(),
    #     attach_to=ego_vehicle)
    # actor_list.append(lane_invasion_sensor)
    
    # Create a named window (optional, but useful for larger images)
    # cv2.namedWindow('Segmentation Map', cv2.WINDOW_NORMAL)

    clock = pygame.time.Clock()
    set_speed = 20  # km/h
    record_data = False
    steer = 0.0
    speed = 0.0
    driving_state = 0

    steering_input_data = []
    rgb_data = np.zeros((14400, camera_params['image_size_y'], camera_params['image_size_x'] ,3), dtype=np.uint8)
    semseg_data = []
    lidar_data = []
    imu_data = []
    driving_state_data = []
    SIM_FPS = 15
    steer_noise_cap = 0.2
    input_noise = generate_temporally_correlated_noise(600, 1/SIM_FPS, steer_noise_cap, 0.05)
    tick_count = 0
    save_recorded_data = False
    VIZ_LIDAR = False
    STEERING_NOISE = True
    # Set up Open3D Visualizer
    if VIZ_LIDAR:
        vis = o3d.visualization.Visualizer()
        vis.create_window()
        point_cloud = o3d.geometry.PointCloud()
        vis.add_geometry(point_cloud)
    else:
        vis = None
        point_cloud = None
    try:
        with CarlaSyncMode(world, ego_vehicle, camera_rgb, camera_semseg, lidar, imu_sensor, fps=SIM_FPS) as sync_mode:
            while True:
                clock.tick(SIM_FPS)  # Limit FPS 
                pygame.event.pump()
                if should_quit(): #Checks if pygame is signalled to quit
                    return
                # Get updated steering input
                # Use the shared steering value
                weather.tick(sync_mode.delta_seconds*10)
                world.set_weather(weather.weather)
                steer = joy.get_axis(0)
                if joy.get_button(2) != 0:
                    driving_state = -1
                    print("Turning Left")
                if joy.get_button(7) != 0:
                    driving_state = 1
                    print("Turning Right")
                if joy.get_button(3) != 0:
                    driving_state = 0
                    print("Going Straight")

                if joy.get_button(6) != 0:
                    if record_data is False:
                        record_data = True 
                        save_recorded_data = True
                        print("RECORDING DATA")
                if joy.get_button(5) != 0:
                    if record_data is True:
                        record_data = False 
                        print("RECORDING STOPPED")
                if joy.get_button(4) != 0:
                    print("Terminating")
                    break
                if STEERING_NOISE:
                    output_steer = steer + input_noise[tick_count]
                else:
                    output_steer = steer
                steer_const_speed(ego_vehicle, speed, set_speed, output_steer)  
                data, velocity = sync_mode.tick(timeout=20.0)

                speed = 3.6 * np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)  # m/s to km/h
                info_text = f"Vel_x:{velocity.x:.2f} m/s, Vel_y: {velocity.y:.2f} m/s, Vel_z:{velocity.z:.2f} m/s, Speed: {speed:.2f} km/h, Steer:{steer:.2f}"
                #print(info_text)

                snapshot, carla_image_rgb, carla_image_semseg, carla_lidar_frame, carla_imu_frame = data

                draw_image(display, carla_image_rgb)
                
                np_image_rgb, np_image_semseg, np_lidar, np_imu = process_image(carla_image_rgb), process_semSeg(carla_image_semseg), process_lidar(carla_lidar_frame, vis, point_cloud), process_imu(carla_imu_frame)

                if record_data:
                    steering_input_data.append(steer)
                    rgb_data[tick_count] = np_image_rgb
                    semseg_data.append(np_image_semseg)
                    lidar_data.append(np_lidar)
                    imu_data.append(np_imu)
                    driving_state_data.append(driving_state)

                #visualize_lidar_top_down(np_lidar)
                #cv2_update_image(np_image_semseg)

                fps = round(1.0 / snapshot.timestamp.delta_seconds)
                display.blit(
                    font.render('% 5d FPS (real)' % clock.get_fps(), True, (255, 255, 255)),
                    (8, 10))
                display.blit(
                    font.render('% 5d FPS (simulated)' % fps, True, (255, 255, 255)),
                    (8, 28))
                tick_count += 1
                pygame.display.flip()
                
    finally:

        print("Cleaning up actors...")
        cv2.destroyAllWindows()
        for actor in actor_list:
            actor.destroy()
        ego_vehicle.destroy()
        pygame.quit()
        vis.destroy_window()
        if save_recorded_data:
            # Create directory for expert data
            map_name = simulation_params['map_name']
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            print("SAVING WITH TIMESTAMP:", timestamp)
            output_dir = os.path.join('expert_data', map_name, timestamp)
            os.makedirs(output_dir, exist_ok=True) 
            # Save the data
            try:
                save_lidar_sequence(lidar_data, os.path.join(output_dir, f'lidar_sequence_{timestamp}_steerNoiseCap_{str(steer_noise_cap)[2:]}'), chunk_size=1000, timestamp=timestamp)
                print("LIDAR saved.")
                # 1. First, save large image data separately
                np.savez_compressed(os.path.join(output_dir, f'semseg_data_{timestamp}_steerNoiseCap_{str(steer_noise_cap)[2:]}.npz'), 
                                semseg_data=np.array(semseg_data, dtype=np.uint8))
                print("Semantic segmentation data saved.")
                
                np.savez_compressed(os.path.join(output_dir, f'imu_data_{timestamp}_steerNoiseCap_{str(steer_noise_cap)[2:]}.npz'), 
                                imu_data=np.array(imu_data))
                print("IMU data saved.")
                # 2. Then save the smaller arrays together
                np.savez_compressed(os.path.join(output_dir, f'control_data_{timestamp}_steerNoiseCap_{str(steer_noise_cap)[2:]}.npz'),
                                steering_input=np.array(steering_input_data),
                                driving_state_data=np.array(driving_state_data))
                save_RGB_tensor(rgb_data, os.path.join(output_dir, f'rgb_data_{timestamp}_steerNoiseCap_{str(steer_noise_cap)[2:]}.npz'))
                print("RGB data saved.")
            except:
                ipdb.set_trace()   

            print("Expert data saved.")
        
if __name__ == "__main__":
    main()
